# ExercÃ­cios de Big Data

âš™ï¸ Requisitos

Docker e Docker Compose

Python 3.10+ (caso rode localmente)

Apache Spark configurado (no container lab ou via docker-compose)

ğŸš€ Como Rodar Localmente

Clone o repositÃ³rio

git clone https://github.com/seu-usuario/pipeline.git
cd pipeline


Suba o ambiente do laboratÃ³rio

docker compose -f docker/docker-compose.grader.yml up -d


Acesse o container

docker exec -it lab bash


Abra o Jupyter Notebook (se aplicÃ¡vel)

http://localhost:8888

ğŸ§© Rodando os ExercÃ­cios Manualmente

Dentro do container ou notebook:

from pyspark.sql import SparkSession
from exercicios import *

spark = SparkSession.builder.getOrCreate()

# Exemplo: criar e visualizar DataFrame
df = ex01_create_df(spark)
df.show()

# Exemplo: salvar CSV no HDFS
ex02_save_csv(spark, "hdfs://namenode:9000/data/ex1.csv")

ğŸ§ª Executando os Testes
Testes UnitÃ¡rios:
pytest tests/unit -v

Testes de IntegraÃ§Ã£o (requer HDFS, Iceberg e Metastore ativos):
pytest tests/integration -v

Rodar todos os testes:
pytest -v

âš™ï¸ Pipeline de AvaliaÃ§Ã£o (GitHub Actions)

O arquivo .github/workflows/grade.yml executa automaticamente:

Build do ambiente Docker (Dockerfile.grader)

InstalaÃ§Ã£o das dependÃªncias

ExecuÃ§Ã£o dos testes (pytest)

GeraÃ§Ã£o da nota e feedback

ğŸ“¦ DependÃªncias Principais (requirements.txt)
pyspark
pytest
pyiceberg


O Spark e o Hive Metastore sÃ£o configurados via Docker Compose.

ğŸ’¡ Dicas Ãšteis

Verifique logs do Hive Metastore:

docker logs hive-metastore


Verifique diretÃ³rios no HDFS:

hdfs dfs -ls /


Verifique tabelas Iceberg:

spark.sql("SHOW TABLES IN lab.db").show()

ğŸ‘¨â€ğŸ’» Autor

Seu Nome
ğŸ’¼ | ğŸ“ Estudante de ADS
ğŸš€ Foco em Big Data, Spark e Arquiteturas DistribuÃ­das